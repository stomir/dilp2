- Does the main approach work?

  - does it generalize well

- Is too much prediates harmful?

- Is deep better than flat?

- Gradient normalization beneficial?
    - 1e2 seems to be good for `even`
  

- Is entropy loss beneficial?
    inverted at the start?

- Entropy loss where there is gradient?

- Distance loss from bad spot?

- Different norms?
    parametrized ≈Åukasiewicz norms from some paper?

- Some loss to prefer one body predicate (for grandparent)?